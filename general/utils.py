import logging
from collections import defaultdict
from itertools import product

from sklearn.model_selection import KFold, train_test_split, StratifiedKFold
import numpy as np
import pandas as pd
import tensorflow as tf
from sklearn.preprocessing import LabelEncoder


class OutputActivation:
    """
    The OutputActivation class functions as an Enum containing the handled
    output activations in the framework. Although transformers and models are
    independent, it is advised that the range to scale continuous features in
    the transformer is synchronized with the output activations. For example,
    if the original data is scaled between -1 and 1, then tanh is a suitable
    action function to apply to continuous features that are generated by the model.
    """

    # continuous
    LINEAR = 0
    TANH = 1
    SIGMOID = 2

    # categorical
    SOFTMAX = 3
    GUMBEL_SOFTMAX = 4


# TODO add gumbel_softmax
def apply_activation(data, output_act):
    if output_act == OutputActivation.LINEAR:
        pass
    elif output_act == OutputActivation.TANH:
        data = tf.tanh(data)
    elif output_act == OutputActivation.SIGMOID:
        data = tf.sigmoid(data)
    elif output_act == OutputActivation.SOFTMAX:
        data = tf.math.softmax(data)
    else:
        raise ValueError("{} is not a handled activation".format(output_act))
    return data


class DataTypes:
    CATEGORICAL = "categorical"
    CONTINUOUS = "continuous"


class MetaDataTypes:
    """
    Continuous data can either be reals or integers.
    """

    INT = "int"
    FLOAT = "float"


def get_metatype(col):
    """
    np.array will store ints as floats. Determines whether a feature in a np.array
    is an integer or not.

    Parameters
    ----------
    col : np.array
    """
    if np.all(col == col.astype(int)):
        return MetaDataTypes.INT
    else:
        return MetaDataTypes.FLOAT


def apply_metatype(col):
    """
    Change the data type of a column in a dataframe.

    Parameters
    ----------
    col : pd.Series
    """
    metatype = get_metatype(col)
    if metatype == MetaDataTypes.FLOAT:
        return col
    elif metatype == MetaDataTypes.INT:
        return col.astype(int)
    else:
        raise ValueError("{} is an unhandled metatype".format(type(col)))


def load_df(
    data_path,
    as_array=False,
    columns_to_drop=None,
    columns_rename_map=None,
    read_csv_kwargs=None,
):
    """
    Given a path, load the specified data.

    Parameters
    ----------
    data_path : str
    as_array : bool (default=False)
    columns_to_drop : list
        List containing column names to drop. These will be strings if the data
        has named columns or ints if the columns are unnamed.
    columns_rename_map : dict
        Mapping between original column name(s) and the new column name(s).
    read_csv_kwargs : dict
        Keyword arguments to pass to pd.read_csv().
    """
    if read_csv_kwargs is None:
        read_csv_kwargs = {}
    data = pd.read_csv(data_path, **read_csv_kwargs)
    if "Unnamed: 0" in list(data.columns):
        data = data.drop(labels=["Unnamed: 0"], axis=1)
    if columns_to_drop:
        data = data.drop(labels=columns_to_drop, axis=1)
    if columns_rename_map:
        data = data.rename(columns=columns_rename_map)

    nan_cols = (data.isnull().sum(0) > 0).values
    if any(nan_cols):
        cols = data.columns
        msg = (
            "The following columns have nan values: {}. Inserting the median for "
            "continuous columns and a dummy string for discrete columns.\n"
            "The generated data will be biased. It is advised that you remove nans "
            "from your data before running experiments.".format(list(cols[nan_cols]))
        )
        logging.warning(msg)
        data.iloc[:, nan_cols] = data.iloc[:, nan_cols].apply(
            lambda x: x.fillna(np.nanmedian(x))
            if x.dtype in [float, int]
            else x.fillna("nan")
        )
    if as_array:
        return data.values
    return data


def train_test_indices(data, test_pct, k_fold=False, stratified_col_idx=None):
    """
    Determine the train/test indices.

    Parameters
    ----------
    data : np.array
    test_pct : float
         Percentage of data to use for the test set.
    k_fold : bool (default=False)
        Whether to return train/test indices based on k-fold CV. The number
        of folds is determined by int(1 / test_pct).
    stratified_col_idx : int (default=None)
        The column to use for stratification in KFoldStratification. The default
        is to use no stratification.

    Returns
    -------
    train_indices : list
    test_indices : list
    """
    if k_fold is False and stratified_col_idx is not None:
        raise ValueError("k_fold must be True in order to use stratified kfold CV")

    if k_fold and test_pct == 0:
        raise ValueError("k_fold must be False if running experiment with no test set")

    train_indices, test_indices = [], []

    if k_fold:
        folds = int(1 / test_pct)
        # cannot shuffle here because the data is re-indexed in evaluator
        if stratified_col_idx is not None:
            y = data[:, stratified_col_idx]
            kf = StratifiedKFold(n_splits=folds, shuffle=False)
            for train_index, test_index in kf.split(np.zeros(len(y)), y):
                train_indices.append(train_index)
                test_indices.append(test_index)
        else:
            kf = KFold(n_splits=folds, shuffle=False)
            for train_index, test_index in kf.split(data):
                train_indices.append(train_index)
                test_indices.append(test_index)
    elif k_fold is False and test_pct != 0.0:
        train_idx, test_idx = train_test_split(
            np.arange(data.shape[0]), test_size=test_pct, shuffle=False
        )
        train_indices.append(train_idx)
        test_indices.append(test_idx)
    else:
        train_indices.append(np.arange(len(data)))
        test_indices.append(None)

    return train_indices, test_indices


def get_categorical_indices(row):
    """
    Find the categorical features in an np.array.

    Parameters
    ----------
    row : np.array

    Returns
    -------
    categorical_indices : np.array
    """
    is_continuous = np.array([isinstance(d, (float, int)) for d in row])
    categorical_indices = np.arange(row.shape[0])[~is_continuous]
    return categorical_indices


def label_encode_categorical_columns(
    data, return_metadata=False, categorical_columns=None, categorical_indices=None
):
    """
    For some data, apply label encoding to the categorical features
    without having them specified beforehand.

    Only one, or neither, of either categorical_columns or categorical_indices
    should be passed.

    Parameters
    ----------
    data : np.array, pd.DataFrame
    return_metadata : bool (default=False)
    categorical_columns : list (default=None)
        The categorical columns of the data. If passed to the method, it assumes
        that all categorical columns are included. If it is not passed, the method
        will identify categorical columns based on the values that are not int or float.
    categorical_indices : list (default=None)
        The indices of the categorical columns.
    """
    if categorical_columns and not isinstance(data, pd.DataFrame):
        raise ValueError("data must be a dataframe if passing categorical columns")
    elif categorical_columns and categorical_indices:
        raise ValueError("only one of categorical columns and indices should be passed")
    elif categorical_columns and isinstance(data, pd.DataFrame):
        cols = list(data.columns)
        categorical_indices = [
            i for i in range(len(cols)) if cols[i] in categorical_columns
        ]
        data = data.values
    else:
        if isinstance(data, pd.DataFrame):
            data = data.values
        if categorical_indices:
            pass
        else:
            categorical_indices = get_categorical_indices(data[0])
            categorical_indices = list(map(int, categorical_indices))

    label_encoder = LabelEncoder()
    for idx in categorical_indices:
        data[:, idx] = label_encoder.fit_transform(data[:, idx])

    categories_per_col = np.max(data[:, categorical_indices], 0) + 1
    num_categories_per_col = list(map(int, categories_per_col))

    # convert from dtype O to float
    data = data.astype(np.float64)

    if return_metadata:
        return [data, categorical_indices, num_categories_per_col]
    else:
        return data


def shuffle_data(data, shuffle_idx=None, return_index=False):
    """
    Apply shuffling to some data.

    Parameters
    ----------
    data : np.array
    shuffle_idx : np.array, list (default=None)
        The indices to use for shuffling. If None is passed then the function
        will generate a new set of shuffle indices.

    return_index : bool (default=False)
        Whether to return the index. Useful if it is generated wtithin in the function.
    """
    if shuffle_idx is None:
        shuffle_idx = np.random.choice(
            np.arange(data.shape[0]), size=data.shape[0], replace=False
        )
    else:
        if not isinstance(shuffle_idx, np.ndarray):
            raise ValueError("shuffle index must be a np.array")
        if shuffle_idx.shape[0] != data.shape[0]:
            raise ValueError("shuffle index must be same length as data")
    data = data[shuffle_idx]
    if return_index:
        return [data, shuffle_idx]
    else:
        return data


def dict_contains_sub_dict(config, test_config):
    """
    Determine whether a dictionary contains a sub-dictionary. Returns True if
    test_config is an empty dict.

    Parameters
    ----------
    config : dict
    test_config : dict
    """
    if not isinstance(config, dict) or not isinstance(test_config, dict):
        raise ValueError("config and test_config both must be dictionaries")

    if bool(test_config) is False:
        return True

    out = []
    for k, v in test_config.items():
        if not all(out):
            break
        if k not in config:
            out.append(False)
            continue
        elif isinstance(v, dict):
            out.append(dict_contains_sub_dict(config[k], test_config[k]))
        else:
            out.append(True if config[k] == v else False)
    return all(out)


class DataPreprocessor:
    def __init__(
        self,
        with_shuffle_data=True,
        with_label_encode_categorical_cols=True,
        shuffle_idx=None,
    ):
        self._with_shuffle_data = with_shuffle_data
        self._with_label_encode_categorical_cols = with_label_encode_categorical_cols

        self.shuffle_idx = shuffle_idx
        self.column_names = None
        self.categorical_indices = None
        self.num_categories_per_col = None

    @staticmethod
    def load_df(data_path, as_array=False):
        data = pd.read_csv(data_path)
        if "Unnamed: 0" in list(data.columns):
            data = data.drop(labels=["Unnamed: 0"], axis=1)
        if as_array:
            return data.values
        return data

    @staticmethod
    def get_categorical_indices(row):
        is_continuous = np.array([isinstance(d, (float, int)) for d in row])
        categorical_indices = np.arange(row.shape[0])[~is_continuous]
        return categorical_indices

    def label_encode_categorical_columns(self, data):
        data = data.values
        categorical_indices = self.get_categorical_indices(data[0])
        self.categorical_indices = list(map(int, categorical_indices))

        label_encoder = LabelEncoder()
        for idx in categorical_indices:
            data[:, idx] = label_encoder.fit_transform(data[:, idx])

        categories_per_col = np.max(data[:, categorical_indices], 0) + 1
        self.num_categories_per_col = list(map(int, categories_per_col))

        # convert from dtype O to float
        data = data.astype(np.float64)
        return data

    def shuffle_data(self, data):
        if self.shuffle_idx is None:
            self.shuffle_idx = np.random.choice(
                np.arange(data.shape[0]), size=data.shape[0], replace=False
            )
        else:
            if not isinstance(self.shuffle_idx, np.ndarray):
                raise ValueError("shuffle index must be a np.array")
            if self.shuffle_idx.shape[0] != data.shape[0]:
                raise ValueError("shuffle index must be same length as data")
        data = data[self.shuffle_idx]
        return data

    def preprocess_data(self, data_path):
        data = self.load_df(data_path)
        self.column_names = list(data.columns)

        if self._with_label_encode_categorical_cols:
            data = self.label_encode_categorical_columns(data)

        if self._with_shuffle_data:
            data = self.shuffle_data(data)
        return data

    @staticmethod
    def train_test_indices(data, k_fold, test_pct):
        train_indices, test_indices = [], []

        if k_fold:
            folds = int(1 / test_pct)
            # cannot shuffle here because the data is re-indexed in evaluator
            kf = KFold(n_splits=folds, shuffle=False)
            for train_index, test_index in kf.split(data):
                train_indices.append(train_index)
                test_indices.append(test_index)
        else:
            train_idx, test_idx = train_test_split(
                np.arange(data.shape[0]), test_size=test_pct
            )
            train_indices.append(train_idx)
            test_indices.append(test_idx)

        return train_indices, test_indices


def subset_indices(data, subset, col_names, include_names=False):
    """
    Based on the subsets passed, this method will identify indices in the data that
    correspond to each subgroup. If no subset is passed it will consider the
    subset to be the entire dataset.

    Parameters
    ----------
    data : np.ndarray
    subset : list of stings
    col_names : list of strings
    include_names : bool (default=False)
    """
    out = {}
    subset_tuples_dict = defaultdict(list)
    if not subset:
        idx = np.ones(data.shape[0], dtype=bool)
        out["all"] = {"idx": idx, "support": np.sum(idx)}
    else:
        for col in subset:
            col_idx = col_names.index(col)
            col_classes = list(set(data[:, col_idx]))
            for col_class in col_classes:
                subset_tuples_dict[col].append((col_idx, col_class))
        subset_groups = list(product(*subset_tuples_dict.values()))
        num_subsets = len(subset)
        for group in subset_groups:
            mask_matrix = np.zeros((data.shape[0], num_subsets))
            for i, subset in enumerate(group):
                mask_matrix[:, i] = data[:, subset[0]] == subset[1]
            idx = np.all(mask_matrix, axis=1)
            subset_dict = {"idx": idx, "support": np.sum(idx)}
            if include_names:
                name = "_".join(
                    ["".join([col_names[v[0]], str(int(v[1]))]) for v in group]
                )
                subset_dict["name"] = name
            out[group] = subset_dict
    return out
